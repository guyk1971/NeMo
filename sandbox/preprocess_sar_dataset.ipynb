{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050b7e2e",
   "metadata": {},
   "source": [
    "# Preprocess SAR dataset\n",
    "the goal of this notebook is to preprocess the SAR dataset to a format that can be consumed by `MegatronGPTModel`.   \n",
    "\n",
    "following the process outlined in [GPT Training Tutorial](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/gpt/gpt_training.html)  \n",
    "\n",
    "- Input (step 1):\n",
    "    - `train_assoc_recall_<n-samples>_<vocab-size>_<contet-length>.pt`\n",
    "    - `test_assoc_recall_<n-samples>_<vocab-size>_<contet-length>.pt`\n",
    "\n",
    "- Step 2: Extract row data - save the files as train_data.jsonl (and test_data.jsonl)\n",
    "\n",
    "- Step 3: Tokenizer - define the `sar-vocab.txt` tokenizer file\n",
    "\n",
    "\n",
    "- step 4: convert training data into memory map format\n",
    "    - generating `gpt_training_data_text_document.bin` and `gpt_training_data_text_document.idx`\n",
    "    these files can be then loaded to the MegatronGPT model for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b1bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gkoren/scratch/code/github/guyk1971/NeMo\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "root_path = os.path.abspath('..')\n",
    "print(root_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b5886",
   "metadata": {},
   "source": [
    "# Step 1: Explore the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb3af5",
   "metadata": {},
   "source": [
    "## safari tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98118aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_assoc_recall_4000_40_1024.pt', 'test_assoc_recall_4000_40_1024.pt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root_path=os.path.join(root_path,'sandbox')\n",
    "ds_files = [f for f in os.listdir(data_root_path) if f.endswith('.pt')]\n",
    "ds_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc14449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 2, 1026])\n",
      "torch.Size([500, 2, 1026])\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "num_examples=4000\n",
    "vocab_size=40\n",
    "input_seq_len=1024\n",
    "train_tensor = torch.load(os.path.join(data_root_path, \n",
    "    f\"train_assoc_recall_{num_examples}_{vocab_size}_{input_seq_len}.pt\"))\n",
    "test_tensor = torch.load(os.path.join(data_root_path, \n",
    "    f\"test_assoc_recall_{num_examples}_{vocab_size}_{input_seq_len}.pt\"))\n",
    "\n",
    "print(train_tensor.shape)\n",
    "print(test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b34e0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6, 22, 15,  ..., 27, 39, 17])\n"
     ]
    }
   ],
   "source": [
    "print(train_tensor[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd847b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 15, 28,  ..., 39, 17, 27])\n"
     ]
    }
   ],
   "source": [
    "print(train_tensor[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f729b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8, 21, 19,  ..., 35, 39, 13])\n"
     ]
    }
   ],
   "source": [
    "print(test_tensor[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1110ac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100, -100, -100,  ..., -100, -100,   35])\n"
     ]
    }
   ],
   "source": [
    "print(test_tensor[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c500f928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1023)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnp=test_tensor.numpy()\n",
    "tnp=np.concatenate((tnp[:,0,4:],tnp[:,1,-1].reshape(-1,1)),axis=-1)\n",
    "tnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2803383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 20,  6, ..., 39, 13, 35])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e515f",
   "metadata": {},
   "source": [
    "## train_data.jsonl\n",
    "`train_data.jsonl` will contain our training data in the json line format. We are interested in the data under `text` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl_file(file_path, num_lines=None):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if num_lines is not None and idx >= num_lines:\n",
    "                break  # Stop reading after reaching the specified number of lines\n",
    "            # Each line is a JSON object, so we parse it and add it to the list\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_files = [f for f in os.listdir(data_root_path) if f.endswith('.jsonl')]\n",
    "print(train_json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f532dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(data_root_path,'train_data.jsonl')\n",
    "num_lines_to_read = 10  # Replace with the number of lines you want to read\n",
    "result = load_jsonl_file(file_path, num_lines_to_read)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36151843",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e4e90",
   "metadata": {},
   "source": [
    "# Step 2: Extract Raw Data (Converting tensor to jsonl)\n",
    "we need to take each sample in the tensor as a document. i.e. `train_tensor[0,0,:]` is equivalent to `result[0]['text']` except for the fact that we need to feed the last token as well (the value of the query). so it looks like the model size should be 1024+3=1027, right ? its not a good number. (although not much worse than 1026, right? maybe we need to pad it further ?)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d95b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp=train_tensor.numpy()\n",
    "# take the list token from tnp[:,1,:] and concat to tnp[:,0,:]\n",
    "tnp=np.concatenate((tnp[:,0,4:],tnp[:,1,-1].reshape(-1,1)),axis=-1)\n",
    "tnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([str(i) for i in tnp[0,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sample to string to create the equivalent to result[:]['text'] that should be saved to json\n",
    "dict2jsn=[{'text':\" \".join([str(i) for i in tnp[j,:]])} for j in range(len(tnp)) ]\n",
    "dict2jsn[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff322e54",
   "metadata": {},
   "source": [
    "write to jsonl file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_jsonl_file(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data:\n",
    "            # Convert each dictionary to a JSON string and write it as a line\n",
    "            json_line = json.dumps(item)\n",
    "            file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(data_root_path,'train_sar.jsonl')\n",
    "write_jsonl_file(file_path, dict2jsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "num_lines_to_read = 10  # Replace with the number of lines you want to read\n",
    "result = load_jsonl_file(file_path, num_lines_to_read)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp[0,-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85e74b",
   "metadata": {},
   "source": [
    "### debugging a sample from the batch (in MegatronGPTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='25 10 33 12 32 10 33 11 22 4 27 7 37 3 37 9 23 9 23 6 29 2 21 17 27 6 29 14 38 17 27 2 21 4 27 2 21 14 38 5 23 16 28 14 38 6 29 14 38 4 27 10 33 18 34 9 23 1 21 4 27 9 23 8 34 13 25 5 23 12 32 12 32 11 22 15 25 5 23 6 29 16 28 13 25 1 21 19 22 10 33 10 33 11 22 10 33 3 37 15 25 19 22 6 29 16 28 12 32 15 25 10 33 7 37 11 22 9 23 4 27 19 22 15 25 15 25 15 25 17 27 2 21 7 37 17 27 9 23 15 25 17 27 4 27 16 28 4 27 8 34 11 22 4 27 13 25 16 28 12 32 3 37 2 21 8 34 18 34 1 21 10 33 15 25 9 23 9 23 15 25 6 29 15 25 14 38 15 25 1 21 11 22 8 34 16 28 4 27 7 37 1 21 10 33 13 25 6 29 3 37 7 37 15 25 19 22 16 28 15 25 15 25 15 25 14 38 14 38 16 28 14 38 5 23 10 33 3 37 18 34 6 29 10 33 13 25 9 23 13 25 1 21 14 38 19 22 16 28 17 27 9 23 15 25 15 25 2 21 7 37 19 22 15 25 10 33 2 21 10 33 1 21 11 22 14 38 5 23 14 38 2 21 11 22 19 22 7 37 2 21 5 23 3 37 4 27 5 23 6 29 17 27 12 32 11 22 7 37 12 32 4 27 12 32 3 37 14 38 2 21 9 23 2 21 3 37 4 27 5 23 1 21 13 25 19 22 4 27 3 37 2 21 8 34 17 27 2 21 12 32 8 34 8 34 18 34 7 37 9 23 10 33 9 23 8 34 4 27 4 27 19 22 3 37 7 37 10 33 5 23 5 23 10 33 16 28 18 34 15 25 12 32 14 38 2 21 10 33 18 34 13 25 6 29 6 29 9 23 11 22 4 27 12 32 12 32 19 22 5 23 3 37 11 22 7 37 4 27 2 21 7 37 7 37 5 23 7 37 8 34 16 28 14 38 16 28 13 25 12 32 4 27 2 21 3 37 2 21 6 29 14 38 9 23 17 27 16 28 9 23 9 23 14 38 18 34 7 37 13 25 9 23 4 27 12 32 3 37 4 27 17 27 19 22 10 33 8 34 5 23 10 33 17 27 17 27 14 38 4 27 18 34 17 27 13 25 2 21 18 34 4 27 15 25 4 27 5 23 16 28 13 25 3 37 4 27 14 38 10 33 5 23 4 27 12 32 19 22 16 28 18 34 19 22 15 25 9 23 9 23 6 29 17 27 3 37 7 37 9 23 7 37 7 37 17 27 15 25 13 25 15 25 15 25 10 33 2 21 6 29 8 34 17 27 7 37 1 21 15 25 1 21 3 37 10 33 19 22 18 34 10 33 13 25 17 27 7 37 1 21 12 32 3 37 5 23 8 34 6 29 13 25 17 27 19 22 2 21 2 21 13 25 3 37 6 29 16 28 8 34 11 22 2 21 6 29 18 34 5 23 13 25 3 37 9 23 5 23 9 23 14 38 16 28 6 29 18 34 16 28 19 22 19 22 1 21 15 25 3 37 14 38 8 34 6 29 16 28 1 21 2 21 19 22 5 23 1 21 8 34 7 37 16 28 16 28 14 38 15 25 6 29 5 23 6 29 6 29 2 21 18 34 1 21 13 25 10 33 11 22 16 28 2 21 3 37 9 23 14 38 15 25 11 22 4 27 1 21 6 29 3 37 16 28 2 21 2 21 5 23 8 34 13 25 10 33 18 34 15 25 12 32 13 25 14 38 14 38 7 37 11 22 39 2 21 13 24 15 35 6 32 11 31 11 31 11 31 9 28 8 36 18 21 10 26 16 20 18 21 11 31 13 24 8 36 13 24 1 35 2 22 2 22 4 26 4 26 11 31 7 37 1 35 19 32 2 22 6 32 17 35 10 26 11 31 7 37 5 27 13 24 8 36 14 34 6 32 18 21 10 26 5 27 5 27 8 36 10 26 11 31 8 36 4 26 2 22 16 20 18 21 16 20 1 35 2 22 12 34 10 26 19 32 6 32 1 35 10 26 9 28 6 32 13 24 17 35 6 32 14 34 12 34 7 37 14 34 12 34 11 31 14 34 16 20 9 28 3 20 11 31 14 34 12 34 8 36 15 35 1 35 12 34 16 20 15 35 8 36 13 24 14 34 15 35 17 35 9 28 16 20 6 32 1 35 11 31 4 26 6 32 3 20 19 32 18 21 9 28 19 32 18 21 5 27 15 35 1 35 16 20 18 21 4 26 13 24 18 21 12 34 3 20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtn=np.array([int(k) for k in txt.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(txtn==39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178acd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp[0,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp[0,[i+1 for i in np.where(tnp[0]==16)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee036c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtn[[i+1 for i in np.where(txtn==8)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86274b0b",
   "metadata": {},
   "source": [
    "# Step 3: Tokenizer\n",
    "The objective is to define the `sar-vocab.txt` that we'll use.  \n",
    "need to look at the `WordTokenizer` and see if the vocabulary fits. \n",
    "specifically, do we need special tokens ? does the 'copy' token have special meaning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 gpt2-vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_file(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write the dictionary to the file using json.dump()\n",
    "        json.dump(data, file, indent=2)  # 'indent' adds pretty formatting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(file_path, vocab):\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write the dictionary to the file using json.dump()\n",
    "        file.write('{\"pad_token\":\"<pad>\",\"eos_token\":\"<eos>\"}'+'\\n')\n",
    "        for key in vocab.keys():\n",
    "            file.write(f'\"{key}\"'+'\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca79ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=40\n",
    "sar_vocab = {str(i):i for i in range(vocab_size)}\n",
    "sar_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37349b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=f'sar-vocab-{vocab_size}.txt'\n",
    "file_path = os.path.join(data_root_path,fname)\n",
    "write_vocab_file(file_path, sar_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db7e18",
   "metadata": {},
   "source": [
    "# Step 4: convert training data into memory map format\n",
    "this should be done using the script they have provided in their tutorial: \n",
    "```\n",
    "python <NeMo_ROOT_FOLDER>/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=train_data.jsonl \\\n",
    "--json-keys=text \\\n",
    "--tokenizer-library=megatron \\\n",
    "--vocab gpt2-vocab.json \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-type GPT2BPETokenizer \\\n",
    "--merge-file gpt2-merges.txt \\\n",
    "--output-prefix=hfbpe_gpt_training_data \\\n",
    "--append-eod \\\n",
    "--workers=32\n",
    "```\n",
    "\n",
    "\n",
    "note that we need to handle the following arguments:\n",
    "- json-keys : what does it mean ?\n",
    "- tokenizer-library : megatron\n",
    "- vocab : we need to feed our own vocab.json\n",
    "- tokenizer-type: we need to set something else. we need WordTokenizer.\n",
    "- merge-file: optional. ignore it\n",
    "- append-eod: what does it mean ? do we need it ?\n",
    "\n",
    "\n",
    "so the command should be: \n",
    "```\n",
    "python <NeMo_ROOT_FOLDER>/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=sandbox/train_sar.jsonl \\\n",
    "--tokenizer-library=megatron \\\n",
    "--vocab-file=sar-vocab-40.txt \\\n",
    "--dataset-impl=mmap \\\n",
    "--tokenizer-type=word \\\n",
    "--output-prefix=sar_gpt_training_data \\\n",
    "--workers=32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5101f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
