import os
import torch
from torch.utils.data import TensorDataset, Dataset, DataLoader
from typing import Dict
import numpy as np
from tqdm import tqdm
from collections import Counter
import json
import argparse
import re



# root_path = os.path.abspath('..')
# print(root_path)


def write_jsonl_file(file_path, data):
    with open(file_path, 'w') as file:
        for item in data:
            # Convert each dictionary to a JSON string and write it as a line
            json_line = json.dumps(item)
            file.write(json_line + '\n')


def write_json_file(file_path, data):
    with open(file_path, 'w') as file:
        # Write the dictionary to the file using json.dump()
        json.dump(data, file, indent=2)  # 'indent' adds pretty formatting (optional)


def write_vocab_file(file_path, vocab):
    with open(file_path, 'w') as file:
        # Write the dictionary to the file using json.dump()
        file.write('{"pad_token":"<pad>","eos_token":"<eos>"}'+'\n')
        for key in vocab.keys():
            file.write(f'"{key}"'+'\n')


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("input_train_fn", type=str, help='path to the input train tensor as generated by safari framework')
    parser.add_argument('--trn_only', action='store_true', help='if set, process only train data. else process test as well' )
    parser.add_argument('--crop',type=int,default=2,help='crop this number of pairs from the beginning of the sequence')
    args = parser.parse_args()
    return args


def main():
    args = get_args()
    print(f'processing {args.input_train_fn}')

    assert os.path.exists(args.input_train_fn), f'File does not exist: {args.input_train_fn}'
    input_path, filename = os.path.split(args.input_train_fn)

    pattern=re.compile(r'(\w+)_assoc_recall_(\d+)_(\d+)_(\d+).pt')
    match=pattern.match(filename)

    split=match.group(1)    # should be 'train'
    n_samples=int(match.group(2))
    vocab_size=int(match.group(3))
    seq_len=int(match.group(4))

    # processing train
    train_tensor = torch.load(args.input_train_fn)
    print(f'loaded tensor shape: {train_tensor.shape}')
    train_file_path = os.path.join(input_path,f'train_sar_{vocab_size}_{seq_len}.jsonl')
    print(f'generating {train_file_path}')
    tnp=train_tensor.numpy()
    crop=2*args.crop    # cropping args.crop pairs of values
    # take the list token from tnp[:,1,:] and concat to tnp[:,0,:] from the bos
    tnp=np.concatenate((tnp[:,0,crop:],tnp[:,1,-1].reshape(-1,1)),axis=-1)
    print(f'train tensor shape: {tnp.shape}')
    # generate train.json
    dict2jsn=[{'text':" ".join([str(i) for i in tnp[j,:]])} for j in range(len(tnp)) ]   
    write_jsonl_file(train_file_path, dict2jsn)

    # generate the tokenizer
    
    sar_vocab = {str(i):i for i in range(vocab_size)}
    fname=f'sar-vocab-{vocab_size}.txt'
    vocab_file_path = os.path.join(input_path,fname)
    write_vocab_file(vocab_file_path, sar_vocab)
    print(f'generating vocab file: {vocab_file_path}')



    if not args.trn_only:
        # process test tensor
        tst_file_name=args.input_train_fn.replace('train','test')
        if os.path.exists(tst_file_name):
            validation_file_path = os.path.join(input_path,f'test_sar_{vocab_size}_{seq_len}.jsonl')
            print(f'generating {validation_file_path}')
            tst_tensor = torch.load(tst_file_name)
            tnp=tst_tensor.numpy()
            tnp=np.concatenate((tnp[:,0,crop:],tnp[:,1,-1].reshape(-1,1)),axis=-1)
            # generate test.json
            dict2jsn=[{'text':" ".join([str(i) for i in tnp[j,:]])} for j in range(len(tnp)) ]
            write_jsonl_file(validation_file_path, dict2jsn)
        else:
            print(f'{tst_file_name} not found. skipping generation')
    
    print(f'done {args.input_train_fn}')
    print('to complete the preparation run the following python script from the project root folder: \n')
    print(f"python scripts/nlp_language_modeling/preprocess_data_for_megatron.py --input={train_file_path} --tokenizer-library=megatron --vocab-file={vocab_file_path} --dataset-impl=mmap --tokenizer-type=word --output-prefix={input_path}/sar_gpt_training_data --workers=32")
    print(f"python scripts/nlp_language_modeling/preprocess_data_for_megatron.py --input={validation_file_path} --tokenizer-library=megatron --vocab-file={vocab_file_path} --dataset-impl=mmap --tokenizer-type=word --output-prefix={input_path}/sar_gpt_validation_data --workers=32")




if __name__ == '__main__':
    main()