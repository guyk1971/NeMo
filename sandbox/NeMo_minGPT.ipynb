{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0eAURFKXdFT"
   },
   "source": [
    "# NeMo minGPT\n",
    "\n",
    "This notebook is based on 01_NeMo_Models.ipynb notebook. without the didactive matarials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81qdv0mPee-j"
   },
   "source": [
    "# The NeMo Model\n",
    "\n",
    "NeMo comes with several state-of-the-art pre-trained Conversational AI models for users to quickly be able to start training and fine-tuning on their own datasets.   \n",
    "\n",
    "In the previous [NeMo Primer](https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/00_NeMo_Primer.ipynb) notebook, we learned how to download pretrained checkpoints with NeMo and we also discussed the fundamental concepts of the NeMo Model. The previous tutorial showed us how to use, modify, save, and restore NeMo Models.\n",
    "\n",
    "In this tutorial we will learn how to develop a non-trivial NeMo model from scratch. This helps us to understand the underlying components and how they interact with the overall PyTorch ecosystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKNftwxzllth"
   },
   "source": [
    "-------\n",
    "At the heart of NeMo lies the concept of the \"Model\". For NeMo developers, a \"Model\" is the neural network(s) as well as all the infrastructure supporting those network(s), wrapped into a singular, cohesive unit. As such, most NeMo models are constructed to contain the following out of the box (note: some NeMo models support additional functionality specific to the domain/use case!) - \n",
    "\n",
    " -  Neural Network architecture - all of the modules that are required for the model.\n",
    "\n",
    " -  Dataset + Data Loaders - all of the components that prepare the data for consumption during training or evaluation.\n",
    "\n",
    " -  Preprocessing + Postprocessing - any of the components that process the datasets so the modules can easily consume them.\n",
    "\n",
    " -  Optimizer + Schedulers - basic defaults that work out of the box and allow further experimentation with ease.\n",
    "\n",
    " - Any other supporting infrastructure - tokenizers, language model configuration, data augmentation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VOoAQT1mipO"
   },
   "source": [
    "# Constructing a NeMo Model\n",
    "\n",
    "NeMo \"Models\" are comprised of a few key components, so let's tackle them one by one. We will attempt to go in the order that's stated above.\n",
    "\n",
    "To make this slightly challenging, let's port a model from the NLP domain this time. Transformers are all the rage, with BERT and his friends from Sesame Street forming the core infrastructure for many NLP tasks. \n",
    "\n",
    "An excellent (yet simple) implementation of one such model - GPT - can be found in the `minGPT` repository - https://github.com/karpathy/minGPT. While the script is short, it explains and succinctly explores all of the core components we expect in a NeMo model, so it's a prime candidate for NeMo! Sidenote: NeMo supports GPT in its NLP collection, and as such, this notebook aims to be an in-depth development walkthrough for such models.\n",
    "\n",
    "In the following notebook, we will attempt to port minGPT to NeMo, and along the way, discuss some core concepts of NeMo itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "piLOgwOPX1FS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nemo\n",
    "from nemo.core import NeuralModule\n",
    "from nemo.core import typecheck\n",
    "from nemo.core.neural_types import NeuralType\n",
    "from nemo.core.neural_types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StMPyg6oCC9B"
   },
   "source": [
    "## Let's create the minGPT components\n",
    "\n",
    "Now that we have a somewhat firm grasp of neural type checking, let's begin porting the minGPT example code. Once again, most of the code will be a direct port from the [minGPT repository](https://github.com/karpathy/minGPT).\n",
    "\n",
    "Here, you will notice one thing. By just changing class imports, one `@typecheck()` on forward, and adding `input_types` and `output_types` (which are also entirely optional!), we are almost entirely done with the PyTorch Lightning port!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "raFkuSRaBAE0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yakGOXrzF1XW"
   },
   "source": [
    "## Creating Element Types\n",
    "\n",
    "Till now, we have used the Neural Types provided by the NeMo core. But we need not be restricted to the pre-defined element types !\n",
    "\n",
    "Users have total flexibility in defining any hierarchy of element types as they please!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ybhLLVyUF0mo"
   },
   "outputs": [],
   "source": [
    "class AttentionType(EncodedRepresentation):\n",
    "  \"\"\"Basic Attention Element Type\"\"\"\n",
    "\n",
    "class SelfAttentionType(AttentionType):\n",
    "  \"\"\"Self Attention Element Type\"\"\"\n",
    "\n",
    "class CausalSelfAttentionType(SelfAttentionType):\n",
    "  \"\"\"Causal Self Attention Element Type\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mONJRMdbZNSE"
   },
   "source": [
    "## Creating the modules\n",
    "\n",
    "Neural Modules are generally top-level modules but can be used at any level of the module hierarchy.\n",
    "\n",
    "For demonstration, we will treat an encoder comprising a block of Causal Self Attention modules as a typed Neural Module. Of course, we can also treat each Causal Self Attention layer itself as a neural module if we require it, but top-level modules are generally preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w4oXpAL_CoDp"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, block_size, n_head, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                     .view(1, 1, block_size, block_size))\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, block_size, n_head, attn_pdrop, resid_pdrop):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, block_size, n_head, attn_pdrop, resid_pdrop)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv0dyrLifkw0"
   },
   "source": [
    "## Building the NeMo Model\n",
    "\n",
    "Since a NeMo Model is comprised of various parts, we are going to iterate on the model step by step inside this notebook. As such, we will have multiple intermediate NeMo \"Models\", which will be partial implementations, and they will inherit each other iteratively.\n",
    "\n",
    "In a complete implementation of a NeMo Model (as found in the NeMo collections), all of these components will generally be found in a single class.\n",
    "\n",
    "Let's start by inheriting `ModelPT` - the core class of a PyTorch NeMo Model, which inherits the PyTorch Lightning Module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxeG-qMrRgNU"
   },
   "source": [
    "-------\n",
    "**Remember**:\n",
    "\n",
    " - The NeMo equivalent of `torch.nn.Module` is the `NeuralModule.\n",
    " - The NeMo equivalent of the `LightningModule` is `ModelPT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0TsfmCYthMux"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as ptl\n",
    "from nemo.core import ModelPT\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCcgn1bajPW8"
   },
   "source": [
    "------\n",
    "Now, let's convert the above easily into a NeMo Model.\n",
    "\n",
    "A NeMo Model constructor generally accepts only two things - \n",
    "\n",
    "1) `cfg`: An OmegaConf DictConfig object that defines precisely the components required by the model to define its neural network architecture, data loader setup, optimizer setup, and any additional components needed for the model itself.\n",
    "\n",
    "2) `trainer`: An optional Trainer from PyTorch Lightning if the NeMo model will be used for training. It can be set after construction (if required) using the `set_trainer` method. For this notebook, we will not be constructing the config for the Trainer object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQMTCB3kz0UA"
   },
   "source": [
    "## Refactoring Neural Modules\n",
    "\n",
    "As we discussed above, Neural Modules are generally higher-level components of the Model and can potentially be replaced by equivalent Neural Modules.\n",
    "\n",
    "As we see above, the embedding modules, deep transformer decoder network, and final decoder layer have all been combined inside the PyTorch Lightning implementation constructor.\n",
    "\n",
    "------\n",
    "\n",
    "However, the final decoder module could have been an RNN instead of a simple Linear layer, or it could have been a 1D-CNN instead.\n",
    "\n",
    "Likewise, the deep transformer decoder could potentially have a different implementation of Self Attention modules.\n",
    "\n",
    "These changes cannot be easily implemented any more inside the above implementation. However, if we refactor these components into their respective NeuralModules, then we can easily replace them with equivalent modules we construct in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJj5sSkX0xHi"
   },
   "source": [
    "### Refactoring the Embedding module\n",
    "\n",
    "Let's first refactor out the embedding module from the above implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uYwMyjqK05RL"
   },
   "outputs": [],
   "source": [
    "class GPTEmbedding(NeuralModule):\n",
    "  def __init__(self, vocab_size: int, n_embd: int, block_size: int, embd_pdrop: float = 0.0):\n",
    "    super().__init__()\n",
    "\n",
    "    # input embedding stem: drop(content + position)\n",
    "    self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "    self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "    self.drop = nn.Dropout(embd_pdrop)\n",
    "\n",
    "  @typecheck()\n",
    "  def forward(self, idx):\n",
    "    b, t = idx.size()\n",
    "    \n",
    "    # forward the GPT model\n",
    "    token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "    position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "    x = self.drop(token_embeddings + position_embeddings)\n",
    "    return x\n",
    "\n",
    "  @property\n",
    "  def input_types(self):\n",
    "    return {\n",
    "        'idx': NeuralType(('B', 'T'), Index())\n",
    "    }\n",
    "\n",
    "  @property\n",
    "  def output_types(self):\n",
    "    return {\n",
    "        'embeddings': NeuralType(('B', 'T', 'C'), EmbeddedTextType())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5rOP6lyOyRt"
   },
   "source": [
    "### Refactoring the Encoder\n",
    "\n",
    "Next, let's refactor the GPT Encoder - which is implemented as a multi layer Transformer (Decoder) network.\n",
    "\n",
    "------\n",
    "It can be noted that we refer to the GPT \"Encoder\" module - but it is constructed by using Transformer \"Decoder\" blocks.\n",
    "\n",
    "***When we discuss Neural Modules - we are discussing an abstract module with a certain input neural type and a certain output neural type.***\n",
    "\n",
    "For us, the GPT \"Encoder\" neural module will accept any  implementation, whose\n",
    "\n",
    "- input neural type is `NeuralType(('B', 'T', 'C'), EmbeddedTextType())`\n",
    "\n",
    "- output type is `NeuralType(('B', 'T', 'C'), EncodedRepresentation())`\n",
    "\n",
    "-----\n",
    "One concrete implementation of such a GPT \"Encoder\" neural module is a Deep Transformer \"Decoder\" network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1QeQnQ_G2PwH"
   },
   "outputs": [],
   "source": [
    "class GPTTransformerEncoder(NeuralModule):\n",
    "  def __init__(self, n_embd: int, block_size: int, n_head: int, n_layer: int, attn_pdrop: float = 0.0, resid_pdrop: float = 0.0):\n",
    "    super().__init__()\n",
    "\n",
    "    self.blocks = nn.Sequential(*[Block(n_embd, block_size, n_head, attn_pdrop, resid_pdrop) \n",
    "                                  for _ in range(n_layer)])\n",
    "    \n",
    "  @typecheck()\n",
    "  def forward(self, embed):\n",
    "    return self.blocks(embed)\n",
    "\n",
    "  @property\n",
    "  def input_types(self):\n",
    "    return {\n",
    "        'embed': NeuralType(('B', 'T', 'C'), EmbeddedTextType())\n",
    "    }\n",
    "\n",
    "  @property\n",
    "  def output_types(self):\n",
    "    return {\n",
    "        'encoding': NeuralType(('B', 'T', 'C'), CausalSelfAttentionType())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmCR3LK3QHum"
   },
   "source": [
    "### Refactoring the Decoder\n",
    "\n",
    "Finally, let's refactor the Decoder - the small one-layer feed-forward network to decode the answer.\n",
    "\n",
    "-------\n",
    "\n",
    "Note an interesting detail - The `input_types` of the Decoder accepts the generic `EncoderRepresentation()`, where as the `neural_type` of the `GPTTransformerEncoder` has the `output_type` of `CausalSelfAttentionType`.\n",
    "\n",
    "This is semantically *not* a mismatch! As you can see above in the inheritance chart, we declare `EncodedRepresentation` -> `AttentionType` -> `SelfAttentionType` -> `CausalSelfAttentionType`. \n",
    "\n",
    "Such an inheritance hierarchy for the `element_type` allows future encoders (which also have a neural output type of at least `EncodedRepresentation`) to be swapped in place of the current GPT Causal Self Attention Encoder while keeping the rest of the NeMo model working just fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VCPUu0EWQIBX"
   },
   "outputs": [],
   "source": [
    "class GPTDecoder(NeuralModule):\n",
    "  def __init__(self, n_embd: int, vocab_size: int):\n",
    "    super().__init__()\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "    self.head = nn.Linear(n_embd, vocab_size, bias=False) # no need for extra bias due to one in ln_f\n",
    "\n",
    "  @typecheck()\n",
    "  def forward(self, encoding):\n",
    "    x = self.ln_f(encoding)\n",
    "    logits = self.head(x)\n",
    "    return logits\n",
    "\n",
    "  @property\n",
    "  def input_types(self):\n",
    "    return {\n",
    "        'encoding': NeuralType(('B', 'T', 'C'), EncodedRepresentation())\n",
    "    }\n",
    "  \n",
    "  @property\n",
    "  def output_types(self):\n",
    "    return {\n",
    "        'logits': NeuralType(('B', 'T', 'C'), LogitsType())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYLMjlW0Sdy1"
   },
   "source": [
    "### Refactoring the NeMo GPT Model\n",
    "\n",
    "Now that we have 3 NeuralModules for the embedding, the encoder, and the decoder, let's refactor the NeMo model to take advantage of this refactor!\n",
    "\n",
    "This time, we inherit from `ModelPT` instead of the general `LightningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZQlmtYU6iDwi"
   },
   "outputs": [],
   "source": [
    "class AbstractNeMoGPT(ModelPT):\n",
    "  def __init__(self, cfg: OmegaConf, trainer: ptl.Trainer = None):\n",
    "      super().__init__(cfg=cfg, trainer=trainer)\n",
    "\n",
    "      # input embedding stem: drop(content + position)\n",
    "      self.embedding = self.from_config_dict(self.cfg.embedding)\n",
    "      # deep transformer: just a sequence of transformer blocks\n",
    "      self.encoder = self.from_config_dict(self.cfg.encoder)\n",
    "      # decoder: at the end one more layernorm and decode the answers\n",
    "      self.decoder = self.from_config_dict(self.cfg.decoder)\n",
    "\n",
    "      self.block_size = self.cfg.embedding.block_size\n",
    "      self.apply(self._init_weights)\n",
    "\n",
    "      print(\"number of parameters: %e\" % self.num_weights)\n",
    "\n",
    "  @typecheck()\n",
    "  def forward(self, idx):\n",
    "      b, t = idx.size()\n",
    "      assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "      # forward the GPT model\n",
    "      # Remember: Only kwargs are allowed !\n",
    "      e = self.embedding(idx=idx)\n",
    "      x = self.encoder(embed=e)\n",
    "      logits = self.decoder(encoding=x)\n",
    "\n",
    "      return logits\n",
    "\n",
    "  def get_block_size(self):\n",
    "      return self.block_size\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "      \"\"\"\n",
    "      Vanilla model initialization:\n",
    "      - all MatMul weights \\in N(0, 0.02) and biases to zero\n",
    "      - all LayerNorm post-normalization scaling set to identity, so weight=1, bias=0\n",
    "      \"\"\"\n",
    "      if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "          module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "          if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "              module.bias.data.zero_()\n",
    "      elif isinstance(module, nn.LayerNorm):\n",
    "          module.bias.data.zero_()\n",
    "          module.weight.data.fill_(1.0)\n",
    "\n",
    "  @property\n",
    "  def input_types(self):\n",
    "    return {\n",
    "        'idx': NeuralType(('B', 'T'), Index())\n",
    "    }\n",
    "\n",
    "  @property\n",
    "  def output_types(self):\n",
    "    return {\n",
    "        'logits': NeuralType(('B', 'T', 'C'), LogitsType())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFRmxWiSmdF3"
   },
   "source": [
    "## Creating a config for a Model\n",
    "\n",
    "At first glance, not much changed compared to the PyTorch Lightning implementation above. Other than the constructor, which now accepts a config, nothing changed at all!\n",
    "\n",
    "NeMo operates on the concept of a NeMo Model being accompanied by a corresponding config dict (instantiated as an OmegaConf object). This enables us to prototype the model by utilizing Hydra rapidly. This includes various other benefits - such as hyperparameter optimization and serialization/deserialization of NeMo models.\n",
    "\n",
    "Let's look at how actually to construct such config objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uygo0BEYjKuj"
   },
   "outputs": [],
   "source": [
    "# model definition args (required)\n",
    "# ================================\n",
    "# vocab_size: int # size of the vocabulary (number of possible tokens)\n",
    "# block_size: int # length of the model's context window in time\n",
    "# n_layer: int # depth of the model; number of Transformer blocks in sequence\n",
    "# n_embd: int # the \"width\" of the model, number of channels in each Transformer\n",
    "# n_head: int # number of heads in each multi-head attention inside each Transformer block  \n",
    "\n",
    "# model definition args (optional)\n",
    "# ================================\n",
    "# embd_pdrop: float = 0.1, # \\in [0,1]: amount of dropout on input embeddings\n",
    "# resid_pdrop: float = 0.1, # \\in [0,1]: amount of dropout in each residual connection\n",
    "# attn_pdrop: float = 0.1, # \\in [0,1]: amount of dropout on the attention matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4sdqRAFop-n"
   },
   "source": [
    "------\n",
    "As we look at the required parameters above, we need a way to tell OmegaConf that these values are currently not set, but the user should set them before we use them.\n",
    "\n",
    "OmegaConf supports such behavior using the `MISSING` value. A similar effect can be achieved in YAML configs by using `???` as a placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xToaWAJUmtX"
   },
   "source": [
    "### Structure of a Model config\n",
    "\n",
    "Let's first create a config for the common components of the model level config -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZCvLdOlMVLy_"
   },
   "outputs": [],
   "source": [
    "from omegaconf import MISSING\n",
    "common_config = OmegaConf.create({\n",
    "    'vocab_size': MISSING,\n",
    "    'block_size': MISSING,\n",
    "    'n_layer': MISSING,\n",
    "    'n_embd': MISSING,\n",
    "    'n_head': MISSING,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8hvdKa4VmCV"
   },
   "source": [
    "-----\n",
    "The model config right now is still being built - it needs to contain a lot more details!\n",
    "\n",
    "A complete Model Config should have the sub-configs of all of its top-level modules as well. This means the configs of the `embedding`, `encoder`, and the `decoder`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-2_QOZyVgrE"
   },
   "source": [
    "### Structure of sub-module config\n",
    "\n",
    "For top-level models, we generally don't change the actual module very often, and instead, primarily change the hyperparameters of that model.\n",
    "\n",
    "So we will make use of `Hydra`'s Class instantiation method - which can easily be accessed via the class method `ModelPT.from_config_dict()`.\n",
    "\n",
    "Let's take a few examples below -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ntsxQKH0pDac"
   },
   "outputs": [],
   "source": [
    "# Let's create a utility for building the class path\n",
    "def get_class_path(cls):\n",
    "  return f'{cls.__module__}.{cls.__name__}'\n",
    "\n",
    "embedding_config = OmegaConf.create({\n",
    "    '_target_': get_class_path(GPTEmbedding),\n",
    "    'vocab_size': '${model.vocab_size}',\n",
    "    'n_embd': '${model.n_embd}',\n",
    "    'block_size': '${model.block_size}',\n",
    "    'embd_pdrop': 0.2\n",
    "})\n",
    "\n",
    "encoder_config = OmegaConf.create({\n",
    "    '_target_': get_class_path(GPTTransformerEncoder),\n",
    "    'n_embd': '${model.n_embd}',\n",
    "    'block_size': '${model.block_size}',\n",
    "    'n_head': '${model.n_head}',\n",
    "    'n_layer': '${model.n_layer}',\n",
    "    'attn_pdrop': 0.2,\n",
    "    'resid_pdrop': 0.2\n",
    "})\n",
    "\n",
    "decoder_config = OmegaConf.create({\n",
    "    '_target_': get_class_path(GPTDecoder),\n",
    "    # n_embd: int, vocab_size: int\n",
    "    'n_embd': '${model.n_embd}',\n",
    "    'vocab_size': '${model.vocab_size}'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtloTqkqWhpl"
   },
   "source": [
    "##### What is `_target_`?\n",
    "--------\n",
    "\n",
    "In the above config, we see a `_target_` in the config. `_target_` is usually a full classpath to the actual class in the python package/user local directory. It is required for Hydra to locate and instantiate the model from its path correctly.\n",
    "\n",
    "So why do we want to set a classpath?\n",
    "\n",
    "In general, when developing models, we don't often change the encoder or the decoder, but we do change the hyperparameters of the encoder and decoder.\n",
    "\n",
    "This notation helps us keep the Model level declaration of the forward step neat and precise. It also logically helps us demark which parts of the model can be easily replaced - in the future, we can easily replace the encoder with some other type of self-attention block or the decoder with an RNN or 1D-CNN neural module (as long as they have the same Neural Type definition as the current blocks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASDmcgE4XtQ4"
   },
   "source": [
    "##### What is the `${}` syntax?\n",
    "-------\n",
    "\n",
    "OmegaConf, and by extension, Hydra, supports Variable Interpolation. As you can see in the `__init__` of embedding, encoder, and decoder neural modules, they often share many parameters between each other.\n",
    "\n",
    "It would become tedious and error-prone to set each of these constructors' values separately in each of the embedding, encoder, and decoder configs.\n",
    "\n",
    "So instead, we define standard keys inside of the `model` level config and then interpolate these values inside of the respective configs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXvEcXGhZi5I"
   },
   "source": [
    "### Attaching the model and module-level configs\n",
    "\n",
    "So now, we have a Model level and per-module level configs for the core components. Sub-module configs generally fall under the \"model\" namespace, but you have the flexibility to define the structure as you require.\n",
    "\n",
    "Let's attach them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c8hvNeB_aDgi"
   },
   "outputs": [],
   "source": [
    "model_config = OmegaConf.create({\n",
    "    'model': common_config\n",
    "})\n",
    "\n",
    "# Then let's attach the sub-module configs\n",
    "model_config.model.embedding = embedding_config\n",
    "model_config.model.encoder = encoder_config\n",
    "model_config.model.decoder = decoder_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIubuFcOpIB0"
   },
   "source": [
    "-----\n",
    "Let's print this config!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2SyKNgp9pG0N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  vocab_size: ???\n",
      "  block_size: ???\n",
      "  n_layer: ???\n",
      "  n_embd: ???\n",
      "  n_head: ???\n",
      "  embedding:\n",
      "    _target_: __main__.GPTEmbedding\n",
      "    vocab_size: ${model.vocab_size}\n",
      "    n_embd: ${model.n_embd}\n",
      "    block_size: ${model.block_size}\n",
      "    embd_pdrop: 0.2\n",
      "  encoder:\n",
      "    _target_: __main__.GPTTransformerEncoder\n",
      "    n_embd: ${model.n_embd}\n",
      "    block_size: ${model.block_size}\n",
      "    n_head: ${model.n_head}\n",
      "    n_layer: ${model.n_layer}\n",
      "    attn_pdrop: 0.2\n",
      "    resid_pdrop: 0.2\n",
      "  decoder:\n",
      "    _target_: __main__.GPTDecoder\n",
      "    n_embd: ${model.n_embd}\n",
      "    vocab_size: ${model.vocab_size}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PAA07EAauCn"
   },
   "source": [
    "-----\n",
    "Wait, why did OmegaConf not fill in the value of the variable interpolation for the configs yet?\n",
    "\n",
    "This is because OmegaConf takes a deferred approach to variable interpolation. First, we fill in temporary values of the required fields (those marked by `???`). Then, to force resolution ahead of time, we can use the following snippet - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0X4C76JyOAnN"
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugxA0TPtbHVZ"
   },
   "outputs": [],
   "source": [
    "# temp_config = copy.deepcopy(model_config)\n",
    "# temp_config.model.vocab_size = 10\n",
    "# temp_config.model.block_size = 4\n",
    "# temp_config.model.n_layer = 1\n",
    "# temp_config.model.n_embd = 32\n",
    "# temp_config.model.n_head = 4\n",
    "\n",
    "# temp_config = OmegaConf.create(OmegaConf.to_container(temp_config, resolve=True))\n",
    "# print(OmegaConf.to_yaml(temp_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V41RFIpEpiOu"
   },
   "source": [
    "-----\n",
    "Now that we have a config, let's try to create an object of the NeMo Model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IIIVi2IfpsJ4"
   },
   "outputs": [],
   "source": [
    "# Let's work on a copy of the model config and update it before we send it into the Model.\n",
    "cfg = copy.deepcopy(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OllBhswPqQXq"
   },
   "outputs": [],
   "source": [
    "# Let's set the values of the config (for some plausible small model)\n",
    "# values taken from nanoGPT/train_shakespeare_char.py \n",
    "cfg.model.vocab_size = 100\n",
    "cfg.model.block_size = 128\n",
    "cfg.model.n_layer = 1\n",
    "cfg.model.n_embd = 32\n",
    "cfg.model.n_head = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QJm2LnTqqcIM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  vocab_size: 100\n",
      "  block_size: 128\n",
      "  n_layer: 1\n",
      "  n_embd: 32\n",
      "  n_head: 4\n",
      "  embedding:\n",
      "    _target_: __main__.GPTEmbedding\n",
      "    vocab_size: ${model.vocab_size}\n",
      "    n_embd: ${model.n_embd}\n",
      "    block_size: ${model.block_size}\n",
      "    embd_pdrop: 0.2\n",
      "  encoder:\n",
      "    _target_: __main__.GPTTransformerEncoder\n",
      "    n_embd: ${model.n_embd}\n",
      "    block_size: ${model.block_size}\n",
      "    n_head: ${model.n_head}\n",
      "    n_layer: ${model.n_layer}\n",
      "    attn_pdrop: 0.2\n",
      "    resid_pdrop: 0.2\n",
      "  decoder:\n",
      "    _target_: __main__.GPTDecoder\n",
      "    n_embd: ${model.n_embd}\n",
      "    vocab_size: ${model.vocab_size}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXOLhpxdq4Ni"
   },
   "source": [
    "-----\n",
    "\n",
    "You will note that we added the `Abstract` tag for a reason to this NeMo Model and that when we try to instantiate it - it raises an error that we need to implement specific methods.\n",
    "\n",
    "1) `setup_training_data` & `setup_validation_data` - All NeMo models should implement two data loaders - the training data loader and the validation data loader. Optionally, they can go one step further and also implement the `setup_test_data` method to add support for evaluating the Model on its own.\n",
    "\n",
    "Why do we enforce this? NeMo Models are meant to be a unified, cohesive object containing the details about the neural network underlying that Model and the data loaders to train, validate, and optionally test those models.\n",
    "\n",
    "In doing so, once the Model is created/deserialized, it would take just a few more steps to train the Model from scratch / fine-tune/evaluate the Model on any data that the user provides, as long as this user-provided dataset is in a format supported by the Dataset / DataLoader that is used by this Model!\n",
    "\n",
    "2) `list_available_models` - This is a utility method to provide a list of pre-trained NeMo models to the user from the cloud.\n",
    "\n",
    "Typically, NeMo models can be easily packaged into a tar file (which we call a .nemo file in the earlier primer notebook). These tar files contain the model config + the pre-trained checkpoint weights of the Model, and can easily be downloaded from some cloud service. \n",
    "\n",
    "For this notebook, we will not be implementing this method.\n",
    "\n",
    "--------\n",
    "Finally, let's create a concrete implementation of the above NeMo Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Vcwi1lO7t7Sm"
   },
   "outputs": [],
   "source": [
    "from nemo.core.classes.common import PretrainedModelInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofUoJ8DDvq_Y"
   },
   "source": [
    "------\n",
    "Now let's try to create an object of the `BasicNeMoGPT` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otvYW4TBxAju"
   },
   "source": [
    "## Setting up train-val-test steps\n",
    "\n",
    "The above `BasicNeMoGPT` Model is a basic PyTorch Lightning Module, with some added functionality - \n",
    "\n",
    "1) Neural Type checks support - as defined in the Model as well as the internal modules.\n",
    "\n",
    "2) Save and restore of the Model (in the trivial case) to a tarfile.\n",
    "\n",
    "But as the Model is right now, it crucially does not support PyTorch Lightning's `Trainer`. As such, while this Model can be called manually, it cannot be easily trained or evaluated by using the PyTorch Lightning framework.\n",
    "\n",
    "------\n",
    "\n",
    "Let's begin adding support for this then -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_7YziAw_Isu"
   },
   "source": [
    "### Setup for Multi Validation and Multi Test data loaders\n",
    "\n",
    "As discussed in the NeMo Primer, NeMo has in-built support for multiple data loaders for validation and test steps. Therefore, as an example of how easy it is to add such support, we include the `multi_validation_epoch_end` and `multi_test_epoch_end` overrides.\n",
    "\n",
    "It is also practically essential to collate results from more than one distributed GPUs, and then aggregate results properly at the end of the epoch. NeMo strictly enforces the correct collation of results, even if you will work on only one device! Future-proofing is baked into the model design for this case!\n",
    "\n",
    "Therefore NeMo provides the above two generic methods to support aggregation and simultaneously support multiple datasets!\n",
    "\n",
    "**Please note, you can prepend your already existing `on_validation_epoch_end` and `on_test_epoch_end` implementations with the `multi_` in the name, and that alone is sufficient to enable multi-dataset and multi-GPU support!**\n",
    "\n",
    "------\n",
    "**Note: To disable multi-dataset support, simply override `on_validation_epoch_end` and `on_test_epoch_end` instead of `multi_validation_epoch_end` and `multi_test_epoch_end`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpfSn-YUh7GK"
   },
   "source": [
    "## Setting up the optimizer / scheduler\n",
    "\n",
    "We are relatively close to reaching feature parity with the MinGPT Model! But we are missing a crucial piece - the optimizer.\n",
    "\n",
    "All NeMo Model's come with a default implementation of `setup_optimization()`, which will parse the provided model config to obtain the `optim` and `sched` sub-configs, and automatically configure the optimizer and scheduler.\n",
    "\n",
    "If training GPT was as simple as plugging in an Adam optimizer over all the parameters with a cosine weight decay schedule, we could do that from the config alone.\n",
    "\n",
    "-------\n",
    "\n",
    "But GPT is not such a trivial model - more specifically, it requires weight decay to be applied to the weight matrices but not to the biases, the embedding matrix, or the LayerNorm layers.\n",
    "\n",
    "We can drop the support that Nemo provides for such special cases and instead utilize the PyTorch Lightning method `configure_optimizers` to perform the same task.\n",
    "\n",
    "-------\n",
    "\n",
    "Note, for NeMo Models; the `configure_optimizers` is implemented as a trivial call to `setup_optimization()` followed by returning the generated optimizer and scheduler! So we can override the `configure_optimizer` method and manage the optimizer creation manually!\n",
    "\n",
    "NeMo's goal is to provide usable defaults for the general case and simply back off to either PyTorch Lightning or PyTorch nn.Module itself in cases when the additional flexibility becomes necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FgXkZQiVjnOv"
   },
   "outputs": [],
   "source": [
    "class BasicNeMoGPT(AbstractNeMoGPT):\n",
    "    \n",
    "    @classmethod\n",
    "    def list_available_models(cls) -> PretrainedModelInfo:\n",
    "        return None\n",
    "\n",
    "    def step_(self, split, batch, batch_idx=None):\n",
    "        idx, targets = batch\n",
    "        logits = self(idx=idx)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        key = 'loss' if split == 'train' else f\"{split}_loss\"\n",
    "        self.log(key, loss)\n",
    "        return {key: loss}\n",
    "\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        return self.step_('train', *args, **kwargs)\n",
    "\n",
    "    def validation_step(self, *args, **kwargs):\n",
    "        return self.step_('val', *args, **kwargs)\n",
    "\n",
    "    def test_step(self, *args, **kwargs):\n",
    "        return self.step_('test', *args, **kwargs)\n",
    "        \n",
    "    # This is useful for multiple validation data loader setup\n",
    "    def multi_validation_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
    "        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'val_loss': val_loss_mean}\n",
    "\n",
    "    # This is useful for multiple test data loader setup\n",
    "    def multi_test_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
    "        test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        return {'test_loss': test_loss_mean}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('embedding.pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": self.cfg.optim.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=self.cfg.optim.lr, betas=self.cfg.optim.betas)\n",
    "        return optimizer\n",
    "    \n",
    "    def setup_training_data(self, train_data_config: OmegaConf):\n",
    "        self._train_dl = None\n",
    "    \n",
    "    def setup_validation_data(self, val_data_config: OmegaConf):\n",
    "        self._validation_dl = None\n",
    "    \n",
    "    def setup_test_data(self, test_data_config: OmegaConf):\n",
    "        self._test_dl = None    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB1kwctv2cYv"
   },
   "source": [
    "-----\n",
    "Now let's setup the config for the optimizer !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5K7zh9Cn2s2u"
   },
   "outputs": [],
   "source": [
    "OmegaConf.set_struct(cfg.model, False)\n",
    "\n",
    "optim_config = OmegaConf.create({\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'betas': [0.9, 0.98]\n",
    "})\n",
    "\n",
    "cfg.model.optim = optim_config\n",
    "\n",
    "OmegaConf.set_struct(cfg.model, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on TinyShakespeare\n",
    "the rest of the notebook assumes we are going to use the TinyShakespeare dataset to train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P31p8ABthsh0"
   },
   "source": [
    "## Setting up the dataset / data loaders\n",
    "\n",
    "You will note that we added the `Abstract` tag for a reason to this NeMo Model and that when we try to instantiate it - it raises an error that we need to implement specific methods.\n",
    "\n",
    "1) `setup_training_data` & `setup_validation_data` - All NeMo models should implement two data loaders - the training data loader and the validation data loader. Optionally, they can go one step further and also implement the `setup_test_data` method to add support for evaluating the Model on its own.\n",
    "\n",
    "Why do we enforce this? NeMo Models are meant to be a unified, cohesive object containing the details about the neural network underlying that Model and the data loaders to train, validate, and optionally test those models.\n",
    "\n",
    "In doing so, once the Model is created/deserialized, it would take just a few more steps to train the Model from scratch / fine-tune/evaluate the Model on any data that the user provides, as long as this user-provided dataset is in a format supported by the Dataset / DataLoader that is used by this Model!\n",
    "\n",
    "2) `list_available_models` - This is a utility method to provide a list of pre-trained NeMo models to the user from the cloud.\n",
    "\n",
    "Typically, NeMo models can be easily packaged into a tar file (which we call a .nemo file in the earlier primer notebook). These tar files contain the model config + the pre-trained checkpoint weights of the Model, and can easily be downloaded from some cloud service. \n",
    "\n",
    "For this notebook, we will not be implementing this method.\n",
    "\n",
    "--------\n",
    "Finally, let's create a concrete implementation of the above NeMo Model!\n",
    "\n",
    "So we were able almost entirely to replicate the MinGPT implementation. \n",
    "\n",
    "Remember, NeMo models should contain all of the logic to load the Dataset and DataLoader for at least the train and validation step.\n",
    "\n",
    "We temporarily provided empty implementations to get around it till now, but let's fill that in now!\n",
    "\n",
    "-------\n",
    "\n",
    "**Note for datasets**: Below, we will show an example using a very small dataset called `tiny_shakespeare`, found at the original [char-rnn repository](https://github.com/karpathy/char-rnn), but practically you could use any text corpus. The one suggested in minGPT is available at http://mattmahoney.net/dc/textdata.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8dlOcZPkxM1"
   },
   "source": [
    "### Creating the Dataset\n",
    "\n",
    "NeMo has Neural Type checking support, even for Datasets! It's just a minor change of the import in most cases and one difference in how we handle `collate_fn`.\n",
    "\n",
    "We could paste the dataset info from minGPT, and you'd only need to make 2 changes!\n",
    "\n",
    "-----\n",
    "In this example, we will be writing a thin subclass over the datasets provided by `nlp` from HuggingFace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "E-fswFkig9t4"
   },
   "outputs": [],
   "source": [
    "from nemo.core import Dataset\n",
    "from torch.utils import data\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-Z8XuPeClGNm"
   },
   "outputs": [],
   "source": [
    "class TinyShakespeareDataset(Dataset):\n",
    "\n",
    "  def __init__(self, data_path, block_size, crop=None, override_vocab=None):\n",
    "\n",
    "      # load the data and crop it appropriately\n",
    "      with open(data_path, 'r') as f:\n",
    "          if crop is None:\n",
    "              data = f.read()\n",
    "          else:\n",
    "              f.seek(crop[0])\n",
    "              data = f.read(crop[1])\n",
    "\n",
    "      # build a vocabulary from data or inherit it\n",
    "      vocab = sorted(list(set(data))) if override_vocab is None else override_vocab\n",
    "\n",
    "      # Add UNK\n",
    "      special_tokens = ['<PAD>', '<UNK>']  # We use just <UNK> and <PAD> in the call, but can add others.\n",
    "      if not override_vocab:\n",
    "        vocab = [*special_tokens, *vocab]  # Update train vocab with special tokens\n",
    "\n",
    "      data_size, vocab_size = len(data), len(vocab)\n",
    "      print('data of crop %s has %d characters, vocab of size %d.' % (str(crop), data_size, vocab_size))\n",
    "      print('Num samples in dataset : %d' % (data_size // block_size))\n",
    "\n",
    "      self.stoi = { ch:i for i,ch in enumerate(vocab) }\n",
    "      self.itos = { i:ch for i,ch in enumerate(vocab) }\n",
    "      self.block_size = block_size\n",
    "      self.vocab_size = vocab_size\n",
    "      self.data = data\n",
    "      self.vocab = vocab\n",
    "      self.special_tokens = special_tokens\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.data) // self.block_size\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "      # attempt to fetch a chunk of (block_size + 1) items, but (block_size) will work too\n",
    "      chunk = self.data[idx*self.block_size : min(len(self.data), (idx+1)*self.block_size + 1)]\n",
    "      # map the string into a sequence of integers\n",
    "      ixes = [self.stoi[s] if s in self.stoi else self.stoi['<UNK>'] for s in chunk ]\n",
    "      # if stars align (last idx and len(self.data) % self.block_size == 0), pad with <PAD>\n",
    "      if len(ixes) < self.block_size + 1:\n",
    "          assert len(ixes) == self.block_size # i believe this is the only way this could happen, make sure\n",
    "          ixes.append(self.stoi['<PAD>'])\n",
    "      dix = torch.tensor(ixes, dtype=torch.long)\n",
    "      return dix[:-1], dix[1:]\n",
    "\n",
    "  @property\n",
    "  def output_types(self):\n",
    "    return {\n",
    "        'input': NeuralType(('B', 'T'), Index()),\n",
    "        'target': NeuralType(('B', 'T'), LabelsType())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MEMR4TcmP5K"
   },
   "source": [
    "------\n",
    "We didn't have to change anything until here. How then is type-checking done? \n",
    "\n",
    "NeMo does type-checking inside of the collate function implementation itself! In this case, it is not necessary to override the `collate_fn` inside the Dataset, but if we did need to override it, **NeMo requires that the private method `_collate_fn` be overridden instead**.\n",
    "\n",
    "We can then use data loaders with minor modifications!\n",
    "\n",
    "**Also, there is no need to implement the `input_types` for Dataset, as they are the ones generating the input for the model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeKXAknenVch"
   },
   "source": [
    "-----\n",
    "Let's prepare the dataset that we are going to use - Tiny Shakespeare from the following codebase [char-rnn](https://github.com/karpathy/char-rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VwsdXtVzo--t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-06 15:01:24--  https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: tiny-shakespeare.txt\n",
      "\n",
      "tiny-shakespeare.tx 100%[===================>]   1.06M  3.67MB/s    in 0.3s    \n",
      "\n",
      "2023-11-06 15:01:25 (3.67 MB/s) - tiny-shakespeare.txt saved [1115394/1115394]\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('tiny-shakespeare.txt'):\n",
    "  !wget https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\n",
    "\n",
    "!head -n 5 tiny-shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bfRL4t9_oS4C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data of crop (0, 1000000) has 1000000 characters, vocab of size 67.\n",
      "Num samples in dataset : 7812\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TinyShakespeareDataset('tiny-shakespeare.txt', cfg.model.block_size, crop=(0,         int(1e6)))\n",
    "# val_dataset   = TinyShakespeareDataset('tiny-shakespeare.txt', cfg.model.block_size, crop=(int(1e6), int(50e3)), override_vocab=train_dataset.vocab)\n",
    "# test_dataset  = TinyShakespeareDataset('tiny-shakespeare.txt', cfg.model.block_size, crop=(int(1.05e6), int(100e3)), override_vocab=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7812"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIlCoZDksEDO"
   },
   "source": [
    "### Setting up dataset/data loader support in the Model\n",
    "\n",
    "So we now know our data loader works. Let's integrate it as part of the Model itself!\n",
    "\n",
    "To do this, we use the three special attributes of the NeMo Model - `self._train_dl`, `self._validation_dl` and `self._test_dl`. Once you construct your DataLoader, place your data loader to these three variables. \n",
    "\n",
    "For multi-data loader support, the same applies! NeMo will automatically handle the management of multiple data loaders for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SVSfIk_-rMSg"
   },
   "outputs": [],
   "source": [
    "class NeMoGPT(BasicNeMoGPT):\n",
    "\n",
    "  def _setup_data_loader(self, cfg):\n",
    "    if self.vocab is None:\n",
    "      override_vocab = None\n",
    "    else:\n",
    "      override_vocab = self.vocab\n",
    "\n",
    "    dataset = TinyShakespeareDataset(\n",
    "        data_path=cfg.data_path,\n",
    "        block_size=cfg.block_size,\n",
    "        crop=tuple(cfg.crop) if 'crop' in cfg else None,\n",
    "        override_vocab=override_vocab\n",
    "    )\n",
    "\n",
    "    if self.vocab is None:\n",
    "      self.vocab = dataset.vocab\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=cfg.shuffle,\n",
    "        collate_fn=dataset.collate_fn,  # <-- this is necessary for type checking\n",
    "        pin_memory=cfg.pin_memory if 'pin_memory' in cfg else False,\n",
    "        num_workers=cfg.num_workers if 'num_workers' in cfg else 0\n",
    "    )\n",
    "  \n",
    "  def setup_training_data(self, train_data_config: OmegaConf):\n",
    "    self.vocab = None\n",
    "    self._train_dl = self._setup_data_loader(train_data_config)\n",
    "\n",
    "    # Save the vocab into a text file for now\n",
    "    with open('vocab.txt', 'w') as f:\n",
    "      for token in self.vocab:\n",
    "        f.write(f\"{token}<SEP>\")\n",
    "    \n",
    "    # This is going to register the file into .nemo!\n",
    "    # When you later use .save_to(), it will copy this file into the tar file.\n",
    "    self.register_artifact('vocab_file', 'vocab.txt')\n",
    "  \n",
    "  def setup_validation_data(self, val_data_config: OmegaConf):\n",
    "    # This is going to try to find the same file, and if it fails, \n",
    "    # it will use the copy in .nemo\n",
    "    vocab_file = self.register_artifact('vocab_file', 'vocab.txt')\n",
    "  \n",
    "    with open(vocab_file, 'r') as f:\n",
    "      vocab = []\n",
    "      vocab = f.read().split('<SEP>')[:-1]  # the -1 here is for the dangling <SEP> token in the file\n",
    "      self.vocab = vocab\n",
    "\n",
    "    self._validation_dl = self._setup_data_loader(val_data_config)\n",
    "  \n",
    "  def setup_test_data(self, test_data_config: OmegaConf):\n",
    "    # This is going to try to find the same file, and if it fails, \n",
    "    # it will use the copy in .nemo\n",
    "    vocab_file = self.register_artifact('vocab_file', 'vocab.txt')\n",
    "\n",
    "    with open(vocab_file, 'r') as f:\n",
    "      vocab = []\n",
    "      vocab = f.read().split('<SEP>')[:-1]  # the -1 here is for the dangling <SEP> token in the file\n",
    "      self.vocab = vocab\n",
    "\n",
    "    self._test_dl = self._setup_data_loader(test_data_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ait4nLtIxS96"
   },
   "source": [
    "### Creating the dataset / dataloader config\n",
    "\n",
    "The final step to setup this model is to add the `train_ds`, `validation_ds` and `test_ds` configs inside the model config!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "C6zcTqJixOOL"
   },
   "outputs": [],
   "source": [
    "OmegaConf.set_struct(cfg.model, False)\n",
    "\n",
    "# Set the data path and update vocabular size\n",
    "cfg.model.data_path = 'tiny-shakespeare.txt'\n",
    "cfg.model.vocab_size = train_dataset.vocab_size\n",
    "cfg.model.block_size = 128\n",
    "\n",
    "cfg.model.n_layer = 6\n",
    "cfg.model.n_embd = 384\n",
    "cfg.model.n_head = 6\n",
    "\n",
    "OmegaConf.set_struct(cfg.model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zlvThf7BysyT"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_ds = OmegaConf.create({\n",
    "    'data_path': '${model.data_path}',\n",
    "    'block_size': '${model.block_size}',\n",
    "    'crop': [0, int(6e5)],\n",
    "    'batch_size': 64,\n",
    "    'shuffle': True,\n",
    "})\n",
    "\n",
    "validation_ds = OmegaConf.create({\n",
    "    'data_path': '${model.data_path}',\n",
    "    'block_size': '${model.block_size}',\n",
    "    'crop': [int(6e5), int(2e5)],\n",
    "    'batch_size': 64,\n",
    "    'shuffle': False,\n",
    "})\n",
    "\n",
    "test_ds = OmegaConf.create({\n",
    "    'data_path': '${model.data_path}',\n",
    "    'block_size': '${model.block_size}',\n",
    "    'crop': [int(8e5), int(1.15e6)],\n",
    "    'batch_size': 64,\n",
    "    'shuffle': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path: ${model.data_path}\n",
      "block_size: ${model.block_size}\n",
      "crop:\n",
      "- 0\n",
      "- 600000\n",
      "batch_size: 64\n",
      "shuffle: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QVVzR6WKyMT5"
   },
   "outputs": [],
   "source": [
    "# Attach to the model config\n",
    "OmegaConf.set_struct(cfg.model, False)\n",
    "\n",
    "cfg.model.train_ds = train_ds\n",
    "cfg.model.validation_ds = validation_ds\n",
    "cfg.model.test_ds = test_ds\n",
    "\n",
    "OmegaConf.set_struct(cfg.model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nd_9_mxS0ET-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  vocab_size: 67\n",
      "  block_size: 128\n",
      "  n_layer: 6\n",
      "  n_embd: 384\n",
      "  n_head: 6\n",
      "  embedding:\n",
      "    _target_: __main__.GPTEmbedding\n",
      "    vocab_size: ${model.vocab_size}\n",
      "    n_embd: ${model.n_embd}\n",
      "    block_size: ${model.block_size}\n",
      "    embd_pdrop: 0.2\n",
      "  encoder:\n",
      "    _target_: __main__.GPTTransformerEncoder\n",
      "    n_embd: ${model.n_embd}\n",
      "    block_size: ${model.block_size}\n",
      "    n_head: ${model.n_head}\n",
      "    n_layer: ${model.n_layer}\n",
      "    attn_pdrop: 0.2\n",
      "    resid_pdrop: 0.2\n",
      "  decoder:\n",
      "    _target_: __main__.GPTDecoder\n",
      "    n_embd: ${model.n_embd}\n",
      "    vocab_size: ${model.vocab_size}\n",
      "  optim:\n",
      "    lr: 0.0003\n",
      "    weight_decay: 0.1\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.98\n",
      "  data_path: tiny-shakespeare.txt\n",
      "  train_ds:\n",
      "    data_path: ${model.data_path}\n",
      "    block_size: ${model.block_size}\n",
      "    crop:\n",
      "    - 0\n",
      "    - 600000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "  validation_ds:\n",
      "    data_path: ${model.data_path}\n",
      "    block_size: ${model.block_size}\n",
      "    crop:\n",
      "    - 600000\n",
      "    - 200000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "  test_ds:\n",
      "    data_path: ${model.data_path}\n",
      "    block_size: ${model.block_size}\n",
      "    crop:\n",
      "    - 800000\n",
      "    - 1150000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the config now !\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dlwSQENU0JxA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-06 15:01:52 modelPT:258] You tried to register an artifact under config key=vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2023-11-06 15:01:52 modelPT:258] You tried to register an artifact under config key=vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data of crop (0, 600000) has 600000 characters, vocab of size 66.\n",
      "Num samples in dataset : 4687\n",
      "data of crop (600000, 200000) has 200000 characters, vocab of size 66.\n",
      "Num samples in dataset : 1562\n",
      "data of crop (800000, 1150000) has 315394 characters, vocab of size 66.\n",
      "Num samples in dataset : 2464\n",
      "number of parameters: 1.074816e+07\n"
     ]
    }
   ],
   "source": [
    "# Let's try creating a model now !\n",
    "model = NeMoGPT(cfg=cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_Mp4bhH0tR1"
   },
   "source": [
    "-----\n",
    "All the data loaders load properly ! Yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "In this section we'll write a function that trains the model.  \n",
    "\n",
    "Start with defining the configuration of the trainer. from `megatron_gpt_config.yaml`:\n",
    "```\n",
    "trainer:\n",
    "  devices: 1\n",
    "  num_nodes: 1\n",
    "  accelerator: gpu\n",
    "  precision: 16\n",
    "  logger: False # logger provided by exp_manager\n",
    "  enable_checkpointing: False\n",
    "  use_distributed_sampler: False\n",
    "  max_epochs: -1 # PTL default. In practice, max_steps will be reached first. \n",
    "  max_steps: 100000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches\n",
    "  log_every_n_steps: 10\n",
    "  val_check_interval: 100\n",
    "  limit_val_batches: 50\n",
    "  limit_test_batches: 500\n",
    "  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models\n",
    "  gradient_clip_val: 1.0\n",
    "  benchmark: False\n",
    "  enable_model_summary: False # default PTL callback for this does not support model parallelism, instead we log manually\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "if torch.cuda.is_available():\n",
    "  accelerator = 'gpu'\n",
    "else:\n",
    "  accelerator = 'cpu'\n",
    "print(accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices: 1\n",
      "num_nodes: 1\n",
      "accelerator: gpu\n",
      "precision: 16\n",
      "max_epochs: 40\n",
      "enable_checkpointing: false\n",
      "use_distributed_sampler: false\n",
      "log_every_n_steps: 10\n",
      "gradient_clip_val: 0.2\n",
      "benchmark: false\n",
      "enable_model_summary: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_config = OmegaConf.create({\n",
    "    'devices': 1,\n",
    "    'num_nodes': 1,\n",
    "    'accelerator': 'gpu',\n",
    "    'precision': 16,\n",
    "    'max_epochs': 40,\n",
    "    'enable_checkpointing': False,\n",
    "    'use_distributed_sampler': False,\n",
    "    # 'max_epochs': -1, # PTL default. In practice, max_steps will be reached first. \n",
    "    # 'max_steps': 100000, # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches\n",
    "    'log_every_n_steps': 10,\n",
    "    # 'val_check_interval': 100,\n",
    "    # 'limit_val_batches': 50,\n",
    "    # 'limit_test_batches': 500,\n",
    "    # 'accumulate_grad_batches': 1, # do not modify, grad acc is automatic for training megatron models\n",
    "    'gradient_clip_val': 0.2,\n",
    "    'benchmark': False,\n",
    "    'enable_model_summary': False, # default PTL callback for this does not support model parallelism, instead we log manually\n",
    "})\n",
    "\n",
    "print(OmegaConf.to_yaml(trainer_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.is_dict(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gkoren/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'vocab_size': '???', 'block_size': '???', 'n_layer': '???', 'n_embd': '???', 'n_head': '???', 'embedding': {'_target_': '__main__.GPTEmbedding', 'vocab_size': '${model.vocab_size}', 'n_embd': '${model.n_embd}', 'block_size': '${model.block_size}', 'embd_pdrop': 0.2}, 'encoder': {'_target_': '__main__.GPTTransformerEncoder', 'n_embd': '${model.n_embd}', 'block_size': '${model.block_size}', 'n_head': '${model.n_head}', 'n_layer': '${model.n_layer}', 'attn_pdrop': 0.2, 'resid_pdrop': 0.2}, 'decoder': {'_target_': '__main__.GPTDecoder', 'n_embd': '${model.n_embd}', 'vocab_size': '${model.vocab_size}'}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-06 15:03:44 nemo_logging:349] /home/gkoren/.local/lib/python3.10/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-06 15:03:44 nemo_logging:349] /home/gkoren/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-11-06 15:03:44 nemo_logging:349] /home/gkoren/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a3ab18eaa2415e8a2cd9f79ff31006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    }
   ],
   "source": [
    "# wandb_logger = ptl.loggers.WandbLogger(\n",
    "#     project=\"nemo-mingpt-shakespeare\"\n",
    "# )\n",
    "# wandb_logger.experiment.config.update(model_config)\n",
    "# wandb_logger.experiment.config.update(trainer_config)\n",
    "\n",
    "# wandb_logger.watch(model, log=\"all\", log_freq=trainer_config.log_every_n_steps)\n",
    "\n",
    "trainer = ptl.Trainer(**trainer_config,logger=None)\n",
    "\n",
    "trainer.fit(model)\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZHDqCyo6uWd"
   },
   "source": [
    "## Evaluate the model - end to end!\n",
    "\n",
    "Now that the data loaders have been set up, all that's left is to train and test the model! We have most of the components required by this model - the train, val and test data loaders, the optimizer, and the type-checked forward step to perform the train-validation-test steps! \n",
    "\n",
    "But training a GPT model from scratch is not the goal of this primer, so instead, let's do a sanity check by merely testing the model for a few steps using random initial weights.\n",
    "\n",
    "The above will ensure that - \n",
    "\n",
    "1) Our data loaders work as intended\n",
    "\n",
    "2) The type checking system assures us that our Neural Modules are performing their forward step correctly.\n",
    "\n",
    "3) The loss is calculated, and therefore the model runs end to end, ultimately supporting PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "johk6Z0e0WEm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tester = ptl.Trainer(devices=1, accelerator=accelerator, logger=None, limit_test_batches=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "oqeeofEr1S8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58e04b2694d47bb8fedda3b0a47ecec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">        Test metric        </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span><span style=\"color: #800080; text-decoration-color: #800080\">    1.8373490571975708     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   1.8373490571975708    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.8373490571975708}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqJy7esrA-Ha"
   },
   "source": [
    "# Saving and restoring models\n",
    "\n",
    "NeMo internally keeps track of the model configuration, as well as the model checkpoints and parameters.\n",
    "\n",
    "As long as your NeMo follows the above general guidelines, you can call the `save_to` and `restore_from` methods to save and restore your models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DksG_-7G1Vbe"
   },
   "outputs": [],
   "source": [
    "model.save_to('gpt_model.nemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhjoFdCnBWVh"
   },
   "outputs": [],
   "source": [
    "!ls -d -- *.nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "567txSF0BYXN"
   },
   "outputs": [],
   "source": [
    "temp_model = NeMoGPT.restore_from('gpt_model.nemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model.setup_multiple_test_data(temp_model.cfg.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  accelerator = 'gpu'\n",
    "else:\n",
    "  accelerator = 'cpu'\n",
    "\n",
    "trainer = ptl.Trainer(devices=1, accelerator=accelerator, limit_test_batches =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2HpKzwKJ_MW"
   },
   "source": [
    "------\n",
    "There we go ! Now our models can be serialized and de-serialized without any issue, even with an external vocab file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjCV5u3_OO7a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_NeMo_Models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
